{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leisure-Codes/Shallow-Neural-Network/blob/Leisure_Codes/Shallow_Neural_Network_For_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ4PJQcStJ3q"
      },
      "source": [
        "## Shallow Neural Network For classifying Species of **IRIS** Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht2p9OsNtv1K"
      },
      "source": [
        "### Step 1 :- *Slicing required Data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXiCteXrtESj"
      },
      "source": [
        "Let's take iris data available from the scikit-learn dataset.\n",
        "\n",
        "1} The dataset consists 50 samples from each of three species of Iris (**Iris setosa, Iris virginica, and Iris versicolor**)\n",
        "\n",
        "2} It has four features, **length** and the **width** of the **sepals** and **petals**, in centimeters.\n",
        "\n",
        "3} Using this feature, let's try to **classify** two species **Iris setosa** and **Iris virginica** by training a simple SNN.\n",
        "\n",
        "4} The first hundred samples in the iris data set correspond to Iris setosa, and Iris virginica mapped as 0 and 1 respectively.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g--N4k5XtBzk"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "#extracting first 100 samples pertaining #to iris setosa and verginica\n",
        "X = iris.data[:100, :4]\n",
        "\n",
        "#actual output\n",
        "Y = iris.target[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIJclL4Juhxa",
        "outputId": "1fae8977-2e51-4d60-8cdb-f2086999fbf1"
      },
      "source": [
        "X_norm=X.reshape(100,4)\n",
        "\n",
        "X_data = X_norm.T\n",
        "\n",
        "Y_data = Y.reshape(1,100)\n",
        "\n",
        "print(X_data.shape)\n",
        "\n",
        "print(Y_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 100)\n",
            "(1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8M9HPPuwLAZ"
      },
      "source": [
        "### Step 2 :- initializing Weights & Bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJUoFFEwQAp"
      },
      "source": [
        "1} Before we start the forward propagation, we need to initialize weights and bias to some random values.\n",
        "\n",
        "2} Since we have four features, we need to have weight vector of shape (4,1) and one bias term of shape (1,1).\n",
        "\n",
        "3} In this case, we initialize all our weights and bias to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04HG2doey3cH"
      },
      "source": [
        "def initialiseNetwork(num_features):\n",
        "\n",
        "  W = np.zeros((num_features, 1))\n",
        "\n",
        "  b = 0\n",
        "\n",
        "  parameters = {\"W\": W, \"b\": b}\n",
        "\n",
        "  return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI-PNBXsy_xh"
      },
      "source": [
        "### Step 3 :- Define Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_NWHBw7zEgx"
      },
      "source": [
        "1} Before going with the forward propagation, we need to define an *activation function* for the neuron.\n",
        "\n",
        "2} Since this is a **binary classification**, let's consider a **sigmoid** function that maps any linear input to values between 0 to 1.\n",
        "\n",
        "3} The sigmoid activation function is implemented as shown in the below code snippet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix9s6UA4zhTC"
      },
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "  return 1/(1 + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w55WpORPzkQx"
      },
      "source": [
        "### Step 4 :- Define Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2w6nt-RzpzA"
      },
      "source": [
        "def forwardPropagation(X, Y, parameters):\n",
        "\n",
        "  W = parameters[\"W\"]\n",
        "\n",
        "  b = parameters[\"b\"]\n",
        "\n",
        "  Z = np.dot(W.T,X) + b\n",
        "\n",
        "  A = sigmoid(Z)\n",
        "\n",
        "  return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-FESxrKztHx"
      },
      "source": [
        "### Step 5 :- Define Cost Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOQFWO9EzzYx"
      },
      "source": [
        "def cost(A, Y, num_samples):\n",
        "\n",
        "  return -1/num_samples *np.sum(Y*np.log(A) + (1-Y)*(np.log(1-A)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WhuydRAz2GJ"
      },
      "source": [
        "### Step 6 :- Backpropogation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM1rBeerz9yh"
      },
      "source": [
        "def backPropagration(X, Y, A, num_samples):\n",
        "\n",
        "  dZ = A - Y\n",
        "\n",
        "  dW = (np.dot(X,dZ.T))/num_samples\n",
        "\n",
        "  db = np.sum(dZ)/num_samples\n",
        "\n",
        "  return dW, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW1XBPkvz6bZ"
      },
      "source": [
        "### Step 7 :- Updating Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAQAymI0H2w"
      },
      "source": [
        "1} Once we have the derivatives, you need to subtract them from original weights and bias.\n",
        "\n",
        "2} While subtracting, we multiply the derivatives with a **learning rate** to have control over the **gradient** at each step of iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waZ0ef8p0bVa"
      },
      "source": [
        "def updateParameters(parameters, dW, db, learning_rate):\n",
        "\n",
        "  W = parameters[\"W\"] - (learning_rate * dW)\n",
        "\n",
        "  b = parameters[\"b\"] - (learning_rate * db)\n",
        "\n",
        "  return {\"W\": W, \"b\": b}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0Nul8e0k95"
      },
      "source": [
        "### Step 8 :- Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bHKjrMG0qNR",
        "outputId": "d31b9dc5-dad7-48f0-dd67-5c0d65623372"
      },
      "source": [
        "def model(X, Y, num_iter, learning_rate):\n",
        "  num_features = X.shape[0]\n",
        "  num_samples = float(X.shape[1])\n",
        "  parameters = initialiseNetwork(num_features)\n",
        "  for i in range(num_iter):\n",
        "    A = forwardPropagation(X, Y, parameters)\n",
        "    if(i%100 == 0):\n",
        "      print(\"cost after {} iteration: {}\".format(i, cost(A, Y, num_samples)))\n",
        "    dW, db = backPropagration(X, Y, A, num_samples)\n",
        "    parameters = updateParameters(parameters, dW, db, learning_rate)\n",
        "  return parameters\n",
        "\n",
        "\n",
        "\n",
        "parameters = model(X_data, Y, 1000, 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost after 0 iteration: 0.6931471805599453\n",
            "cost after 100 iteration: 0.06656095976383733\n",
            "cost after 200 iteration: 0.03492693805012102\n",
            "cost after 300 iteration: 0.023931762897153323\n",
            "cost after 400 iteration: 0.018316011387467388\n",
            "cost after 500 iteration: 0.014894365777233853\n",
            "cost after 600 iteration: 0.01258464419969677\n",
            "cost after 700 iteration: 0.010917147703633787\n",
            "cost after 800 iteration: 0.0096547082608724\n",
            "cost after 900 iteration: 0.008664477279333904\n"
          ]
        }
      ]
    }
  ]
}