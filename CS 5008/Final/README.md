# Research Paper
Name: Michael Mills
Semester: Summer 2023
Topic: Combinatorial Optimization Algorithms
Link The Repository:

## Introduction
For my final project, I decided to implement a few algorithms that are directly related to a project that I have been working on for the greater part of the year. I have been working on an application that aims to improve the way that we are able to access files. Currently, it is often the case that we will subscribe to some cloud storage service. This costs a lot of money, and the files are also stored in some unknown location. This project aims to decentralize cloud storage, so we can seamlessly access our files across all of our devices, without the need for some expensive cloud subscription plan. The cool thing about this project is that there are many strategies that can be used to determine what would be the best way to store all of these files. Should we store all of the files on just one device and leave the rest empty? Should we store the same file on multiple different devices? Should we split up files to multiple different devices, so device has one entire file? These problems are where my algorithms are able to come into play. 

For this particular problem, I decided to look at Combinatorial Optimization Algorithms. These types of algorithms are beneficial for when we need to find an optimal solution from a finite number of combinations. In other words, there are a large number of combinations that are available, however which one would happen to be the best? There are many famous problems that are associated with this type of algorithm, such as the Traveling Salesman Problem, the Minimum Spanning Tree Problem, and the Knapsack Problem. 

[Source: "The History of Combinatorial Optimization" by Alexander Schrijver, Handbook of Combinatorial Optimization, Volume 1, 1995]

Algorithms used to solve many various Combinatorial Optimization problems have mostly been prevalent since the early 1900s. Despite most being mostly prevalent in the 1900s, the very first attempt at solving a similar problem occured in 1736. This is when a man by the name of Leonhard Euler worked on the Seven Bridges of Konigsberg Problem. This was a problem where a city consisted of seven bridges, and Euler wanted to figure out how he could create a walk through the city that would allow the traveler to go over each of the bridges just one time.

The Seven Bridges of KÃ¶nigsberg: https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg


In 1931, a man by the name of Denes Konig worked on a problem where there are a certain number of tasks, and a certain number of workers, and only one task should be assigned to one worker. In order to solve this problem Denes came up with the Konig-Egervary theorem. This theorem essentially explains that the maximum number of tasks that can be completed is equal to the minimum number of workers that are needed to complete all of the tasks. 

The Konig-Egervary theorem: https://en.wikipedia.org/wiki/K%C5%91nig%E2%80%93Egerv%C3%A1ry_theorem

In 1947, a man by the name of George Dantzig worked on solving various linear programming problems. While working as a research assistant, he was given the task to solve a certain linear programming problem. He realized that he could solve the problem through an algorithm called the Simplex Method. The simplex method is essentially starting off with a solution that works, and then iteratively coming up with a better solution until the solution becomes optimal. 
George Dantzig: https://en.wikipedia.org/wiki/George_Dantzig

In the 1950s and 1960s, there was a sharp increase in popularity in solving something called the traveling salesman problem. The traveling salesman problem is essentially a problem in which there is a salesman that needs to go to various locations, and there are multiple different orders in which the salesman may visit each city. For example, should he go from Boston to New York, or should he stop in Philadelphia before he goes to New York? This problem, and the subsequent algorithms that were developed for it aim to determine the best possible route for the traveling salesman. 
http://en.wikipedia.org/wiki/Travelling_salesman_problem


Another popular problem is the minimum spanning tree problem. This is a problem that aims to determine a tree of a graph, while maintaining the smallest total edge weight possible. This problem is particularly important in a number of different situations. For example, it can be incredibly helpful when we are thinking about and designing efficient networks. These networks don't just have to be computer networks, they can be any type of network, such as a road system when planning a new city. 

Another popular problem in the realm of ocmputer science is the Knapsack Problem. This was mostly prevalent in the late 1950s and 1960s. The Knapsack Problem is essentially a problem in which there are a certain amount of items that consit of various weights. We are looking to find the best way to pack the knapsack. This problem works by implementing a dynamic programming approach. Additionally, this problem is mostly solved using a greedy approach, which means that each item is determined one at a time, and there is no backtracking involved. 
The knapsack problem: https://en.wikipedia.org/wiki/Knapsack_problem

Now that we have learned about the history of Combinatorial Optimization Algorithms and their history, it is time to give a brief summary of what the rest of this paper will look like. To begin, I will be giving an analysis of the Algorithm, including the time complexity, space complexity, and general analysis. Next, I will be providing an empirical analysis. Next, I will discuss the application of the algorithm, including what it's used for, some real world examples of how its used, and why it can be helpful. Next, I will discuss my implementation of the algorithm, the language that I used, the libraries I used, some challenges I faced, and finally some key points. 

Finally, I will provide an overall summary of what I found, what I learned, and how it helped me become a better computer scientist. 

## Analysis of Algorithm/Datastructure

For these algorithms, there are multiple different parts of each algorithm that have different time complexities. In other words, each algorithm can be broken down into 3 different sections, sorting the files, sorting the devices, and the nested loop. As for sorting the files, this will normally have a time complexity of O(n log n). In this particular scenario, n will be the number of files. As for sorting the devices, this will normally have a time complexity of O(m log m). In this particular scenario, m will be the number of devices. Finally, as for the nested loop, this will normally have a time complexity of O(n * m) where again n is the number of files and m is the number of devices. Since the overall algorithm is a combination of these smaller pieeces, we are able to add all three of these time complexities together, which would be O(n log n + m log m + n * m). We are then able to simplify this to an overall time complexity of O(n * m). 

As for the space complexity, there doesn't seem to be much wrong with the algorithm. I certainly learned a lot in this part of the project, as this was the first time that I had worked with analyzing memory in a python program. I ended up copying all of the algorithm functions to the test_space_complexity.py file, as the memory profiler doesn't review the memory of the function that is called from another file. For the most part, it seems as if the the memory is mostly sound. Of course, there are variables that are assigned that will require memory. In my experience, even with incredibly high numbers for the number of files, number of devices, etc., the memory does not seem to be bad at all. In fact, I thought there was something wrong with the profiler as it was showing zeroes across the board, and I had to increase the workload in order to get it to show anything. 

Overall I think that this is a very interesting algorithm, and probably has a lot of use cases. I think overall the algorithm works especially with large workloads and large datasets specifically because it is considered to be a greedy algorithm. In other words, the algorithm doesn't backtrack and look to change which knapsack to store the file in after it has already been placed. I think this is the sole reason why it is able to work with large numbers. 
## Empirical Analysis

## Application

The algorithms I wrote are primarily used to determine the optimal allocation of files across devices. The algorithms have a number of different variables such as priority, duplication of files across multiple devices, and splitting files into pieces across devices. For example, say for instance that you have a word document that you open for work every single day. This is an important document that requires the highest level of priority. Now say for instance you also have your wedding video, which is still a really important file, but you don't think you'll be using it as frequently as that document you pull up for work every day. In terms of devices, you have a laptop, desktop, Apple Watch, iphone, two Roku devices, an iPad, and a Meta Quest 2. You live in the woods so you never have service with you iphone, but you have killer wifi speeds that your desktop is connected to 24/7. With that being said, this algorithm will determine the optimal allocation for these two files, where it may upload the word document to a large number of devices so it will increase download speeds for when you need it every morning, but it is going to store the wedding video on your iphone that isn't all that reliable in terms of service, but that's okay because you probably won't watch it for another three months. 

There are many other examples of Combinatorial Optimization Algorithms. For example, Google Maps could use an algorithm to determine the fastest point from A to B. Another example is Amazon. Amazon could certainly use an algorithm like this when shipping products. It could be incredibly beneficial to determine what products they should ship together, and which ones would be more profitable to ship separately. Is it cheaper to ship one particular package to Philadelphia before going to New York? Or is it cheaper to go straight to New York. 

## Implementation

For this project, I decided to implement most if not all of my work in Python. I chose Python for a number of reasons. One of which is due to the fact that I don't have to worry about memory management. This makes life much easier for when I am writing code, as I don't have to constantly think about allocating and freeing memory. I also think that Python is just an easier language to use. The syntax makes more sense and is incredibly simple. Python seems to be one of the only languages where I may not know exactly what the syntax is for a particular line of code, and when I just randomly guess what it might be, I have someone of a decent chance of being correct. Another reason why I enjoy Python is because I don't have to worry about compilers and compiling all of my code. I can just simply run the python scripts and it works. It doesn't seem like a lot, but in reality it is twice the amount of keystrokes to compile a project before you run it, and that can add up. 

As for the libraries I used, I really did not use any libraries. I found this one repository on github that I could have used that implemented all of the algorithms automatically without having to write them all out, but I decided not to use that. I also seemed to have some very specific and custom requirements for some of the algorithms that I wrote. One library that I did use, which is not incredibly important is the tkinter library. I basically just used this as a GUI to go ahead an select which repositories or files I am interested in uploading and scanning. This just eliminates the need to manually write out the file path of what directory you are looking to scan. 

One challenge I faced was uploading large directories. I essentially designed the program in a way where the program would go through each individual level of folders from a top down approach until all of the files have been scanned. This means that each individual file is given a CID, and information about the CID's are stored in a directory on your computer. The problem was that the process of uploading files to IPFS Seemed to slow down after 15,000 files. My immediate solution to this problem is to not give a CID to every single file on the computer, and only give CID's to directories, which seems to be possible. I am interested in going into this deeper to determine if there is anything else causing it to slow down. Another problem I had was related to actually developing the algorithms. I had many issues/bugs in many different instances. One of which was the fact that it would only pack a few files in the "knapsack," and leave a large majority of the files unpacked, despite the fact that there was still space left. Another issue I had was the fact that the file sizes would go to some crazy decimal number. This caused the algorithm to just leave those files alone and not place them in a "knapsack." Rounding the file size to a certain decimal fixed this. 

Overall, I really enjoyed working on these algorithms. I am interested to see how I can improve these algorithms in the future. I feel as if the base model of the algorithm, where it just simply places files in devices almost mindlessly seems to be a little bit of a waste. However, I think once I started to incorporate different variables into the algorithm, that's when it really started to get interesting. I think theres a lot of room for improvement as well. For example, we could take into account the frequency that a file is visited or used. We could also take into account how fast the internet is for each individual device. If one particular device has terrible internet speed, you could shift all of the files or some of the files to another device that may be faster. I also think the priorty functionality is a really interesting concept as well, and could influence how many devices a particular file is on. I think the most interesting part of all, is how I would be able to implement all of these variables into one algorithm.

## Summary

Overall, I learned a lot not only with this project but also in this class. Before this class, I had a good general understanding of algorithms and why they are important in the realm of computer science. Taking this class helped make me more aware of the concept of data structures and algorithms within applications, and how important they can be. I heard people talk about time complexities such as O(n) before this class, but now I have a solid understanding of what it means. 